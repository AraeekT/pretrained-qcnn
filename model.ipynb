{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1199870,"sourceType":"datasetVersion","datasetId":615374}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchviz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install qiskit\n!pip install qiskit-aer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install qiskit_machine_learning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install qiskit-algorithms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torchvision.transforms import ToTensor\nfrom torchvision.models import vgg16, VGG16_Weights\nfrom torch import cat, no_grad, manual_seed\nfrom torchvision import datasets, transforms\nimport torch.optim as optim\nfrom torch.nn import (Module, Conv2d, Linear, Dropout2d, CrossEntropyLoss, MaxPool2d, Flatten, Sequential, ReLU)\nfrom torch.autograd import Function\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import Tensor\nfrom torchviz import make_dot\nimport torchvision\nfrom torchvision.io import read_image\nfrom torch.autograd import Variable\nimport qiskit\nfrom qiskit import transpile, assemble\nfrom qiskit.visualization import *\nfrom qiskit import QuantumCircuit\nimport qiskit_aer\nimport qiskit.primitives\nimport qiskit.compiler\nimport qiskit.quantum_info\nfrom qiskit.circuit import Parameter\nfrom qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\nfrom qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\nfrom qiskit_machine_learning.connectors import TorchConnector\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom qiskit.primitives import Estimator\nfrom qiskit.quantum_info import Pauli\nimport torchmetrics\nimport qiskit_algorithms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map = ZZFeatureMap(feature_dimension=2, entanglement='linear')\nansatz = RealAmplitudes(2, reps=1, entanglement='linear')\ncircuit = QuantumCircuit(2)\ncircuit.compose(feature_map, range(2), inplace=True)\ncircuit.compose(ansatz, range(2), inplace=True)\nqnn2 = EstimatorQNN(circuit=circuit.decompose(), input_gradients=True, input_params=feature_map.parameters,\n    weight_params=ansatz.parameters,)\nprint(qnn2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(Module):\n\n    def __init__(self):\n        super().__init__()\n        # Load pre-trained VGG16 model\n        pretrained_model = vgg16(weights=VGG16_Weights.DEFAULT)\n\n        # Freeze the pre-trained model\n        for param in pretrained_model.parameters():\n            param.requires_grad = False\n\n        # Define the feature extractor\n        self.feature_extractor = nn.Sequential(*list(pretrained_model.children())[:-2])\n\n        # Replace first two convolutional layers with extracted features\n        self.conv1 = self.feature_extractor\n        self.conv2 = Conv2d(1280, 32, kernel_size=5)\n\n        # Remaining convolutional layers\n        self.conv3 = Conv2d(32, 64, kernel_size=2)\n        self.conv4 = Conv2d(64, 128, kernel_size=1)\n        self.conv5 = Conv2d(128, 256, kernel_size=1)\n\n        # Dropout layer\n        self.dropout1 = Dropout2d(0.5)\n        self.dropout2 = Dropout2d(0.5)\n\n        # Fully connected layers\n        self.fc1 = Linear(256, 64)\n        self.fc2 = Linear(64, 2)\n\n        # QNN\n        self.qnn = TorchConnector(qnn2)\n\n        # Final layer\n        self.fc3 = Linear(1, 1)\n\n    def forward(self, x):\n        features = self.conv1(x)\n        x = F.relu(self.conv2(features))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv3(x))\n        x = self.dropout1(x)\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = self.dropout2(x)\n        x = x.view(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.qnn(x)  # apply QNN\n        x = self.fc3(x)\n        return torch.cat((x, 1 - x), -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = torchvision.datasets.ImageFolder(root=\"/kaggle/input/sarscov2-ctscan-dataset/\")\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = [\"non-COVID\", \"COVID\"]\n        self.image_folder = torchvision.datasets.ImageFolder(root=self.root_dir, transform=self.transform)\n\n    def __len__(self):\n        return len(self.image_folder)\n\n    def __getitem__(self, idx):\n        img, label = self.image_folder[idx]\n        adjusted_label = 0 if label == 0 else 1\n        return img, adjusted_label\n\n# Define the transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n])\n\n# Create the custom dataset\ndataset = CustomDataset(root_dir=\"/kaggle/input/sarscov2-ctscan-dataset/\", transform=transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset.__len__())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, test_dataset = train_test_split(dataset, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Net()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\n\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics\nimport matplotlib.pyplot as plt\n\n# ... (model, device, train_loader, test_loader definitions) ...\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nloss_func = nn.CrossEntropyLoss()\n\naccuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\nconfusion_matrix = torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=2).to(device)\n\n\nepochs = 50\ntrain_loss_list = []\ntrain_accuracy_list = []\nval_loss_list = []\nval_accuracy_list = []\n\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n\n    train_total_loss = []\n    accuracy.reset()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n        \n        # Forward pass\n        output = model(data)\n\n        # Calculate training loss\n        loss = loss_func(output, target)\n\n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_total_loss.append(loss.item())\n        accuracy.update(output, target)\n\n    # Compute and store training epoch metrics\n    epoch_train_accuracy = accuracy.compute().item()\n    epoch_train_loss = sum(train_total_loss) / len(train_total_loss)\n\n    train_accuracy_list.append(epoch_train_accuracy)\n    train_loss_list.append(epoch_train_loss)\n\n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n\n    val_total_loss = []\n    accuracy.reset()\n    confusion_matrix.reset()\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data = data.to(device)\n            target = target.to(device)\n            # Forward pass\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True).squeeze()\n            # Calculate validation loss\n            loss = loss_func(output, target)\n\n            val_total_loss.append(loss.item())\n            accuracy.update(output, target)# Assuming pred already calculated\n            confusion_matrix.update(pred, target)\n\n    # Compute and store validation epoch metrics\n    epoch_val_accuracy = accuracy.compute().item()\n    epoch_val_loss = sum(val_total_loss) / len(val_total_loss)\n\n    val_accuracy_list.append(epoch_val_accuracy)\n    val_loss_list.append(epoch_val_loss)\n\n    # Print epoch results\n    print(f'Epoch [{epoch + 1}/{epochs}]\\tTrain Loss: {epoch_train_loss:.4f}\\tTrain Accuracy: {epoch_train_accuracy:.4f}\\tVal Loss: {epoch_val_loss:.4f}\\tVal Accuracy: {epoch_val_accuracy:.4f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training and validation metrics\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_accuracy_list, label='Train Accuracy')\nplt.plot(val_accuracy_list, label='Val Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy per Epoch')\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(train_loss_list, label='Train Loss')\nplt.plot(val_loss_list, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss per Epoch')\nplt.grid(True)\nplt.legend()\n\nplt.show()\n\ncm = confusion_matrix.compute()\nprint(\"Confusion Matrix:\\n\", cm)\nfig_, ax_ = confusion_matrix.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nn_samples_show = 5\ncount = 0\nfig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(15, 5))\n\nmodel.eval()\nwith no_grad():\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data = data.to(device)\n        target = target.to(device)\n        if count == n_samples_show:\n            break\n        output = model(data[0:1])\n        if len(output.shape) == 1:\n            output = output.reshape(1, *output.shape)\n\n        pred = output.argmax(dim=1, keepdim=True)\n        axes[count].imshow(data[0].cpu().numpy().squeeze(), cmap='gray')\n\n        axes[count].set_xticks([])\n        axes[count].set_yticks([])\n        if pred.item() == 0:\n            axes[count].set_title('Predicted item: non-COVID')  \n        elif pred.item() == 1:\n            axes[count].set_title('Predicted item: COVID')  \n        count += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
